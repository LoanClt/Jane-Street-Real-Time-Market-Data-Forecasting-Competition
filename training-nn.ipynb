{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on AutoDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_path = 'autodl-tmp/train.parquet'\n",
    "\n",
    "feature_cols = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "target_col = 'responder_6'\n",
    "weight_col = 'weight'\n",
    "\n",
    "df = pl.scan_parquet(train_path).collect().to_pandas()\n",
    "valid = pl.scan_parquet(train_path).filter(pl.col('date_id')>=1650).collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)] \\\n",
    "            + [f\"responder_{idx}_last_lag\" for idx in range(9)] \n",
    "        #+ [f\"feature_{idx:02d}_mean_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_std_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_max_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_min_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_first_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_last_lag\" for idx in range(79)] \\\n",
    "        #+ [f\"feature_{idx:02d}_chg_lag\" for idx in range(79)] \\\n",
    "\n",
    "        #+ [f\"responder_{idx}_mean_lag\" for idx in range(9)] \\\n",
    "        #+ [f\"responder_{idx}_std_lag\" for idx in range(9)] \\\n",
    "        #+ [f\"responder_{idx}_max_lag\" for idx in range(9)] \\\n",
    "        #+ [f\"responder_{idx}_last_lag\" for idx in range(9)] \\\n",
    "        #+ [f\"responder_{idx}_chg_lag\" for idx in range(9)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = df[ xgb_feature_cols ]\n",
    "y_train = df[ target_col ]\n",
    "w_train = df[ weight_col ]\n",
    "\n",
    "X_valid = valid[ xgb_feature_cols ]\n",
    "y_valid = valid[ target_col ]\n",
    "w_valid = valid[ weight_col ]\n",
    "\n",
    "X_train.shape, y_train.shape, w_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import pandas as pd\n",
    "import polars\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.usegpu = True\n",
    "        self.seed = 55\n",
    "        self.model = 'nn'\n",
    "        self.use_wandb = False #Weights & Biases\n",
    "        self.project = 'js-xs-nn-with-lags'\n",
    "        self.dname = \"./input_df/\"\n",
    "        self.loader_workers = 4 #线程数\n",
    "        self.bs = 8192 #Batch Size\n",
    "        self.lr = 1e-3 # 学习率\n",
    "        self.weight_decay = 5e-4 # L2正则化系数\n",
    "        self.dropouts = [0.1, 0.1] # 循环中的两层dropout rate\n",
    "        self.n_hidden = [512, 512, 256] # 循环中的三层线形层的维度\n",
    "        self.patience = 25 #早停\n",
    "        self.max_epochs = 2000 \n",
    "        self.N_fold = 5 #5折交叉验证\n",
    "        self.gpuid = 0\n",
    "\n",
    "\n",
    "my_args = custom_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, accelerator):\n",
    "        self.features = torch.FloatTensor(df[xgb_feature_cols].values).to(accelerator)\n",
    "        self.labels = torch.FloatTensor(df[target_col].values).to(accelerator)\n",
    "        self.weights = torch.FloatTensor(df[weight_col].values).to(accelerator)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n",
    "        super().__init__()\n",
    "        self.df = train_df\n",
    "        self.batch_size = batch_size\n",
    "        self.dates = self.df['date_id'].unique()\n",
    "        self.accelerator = accelerator\n",
    "        self.train_dataset = None\n",
    "        self.valid_df = None\n",
    "        if valid_df is not None:\n",
    "            self.valid_df = valid_df\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self, fold=0, N_fold=3, stage=None):\n",
    "        # Split dataset\n",
    "        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n",
    "        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n",
    "        self.train_dataset = CustomDataset(df_train, self.accelerator)\n",
    "        \n",
    "        # 如果又独立的验证集\n",
    "        if self.valid_df is not None:\n",
    "            df_valid = self.valid_df\n",
    "            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n",
    "\n",
    "    #shuffle：是否在每个epoch开始时打乱数据顺序，避免过拟合\n",
    "    def train_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n",
    "\n",
    "    #时间序列需要按时间顺序评估，验证集保持shaffle = false\n",
    "    def val_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return r2\n",
    "\n",
    "#LightningModule 是一个神经网络类，标准化了模型训练、验证和优化的流程\n",
    "'''\n",
    "神经网络结构如下：\n",
    "Input(0)→BN(0)→Linear(0→1)→BN(1)→SiLU(1)→Dropout(1)→Linear(1→2)→...→Linear→Tanh→Output\n",
    "循环中，除了第0层外，都加入SiLU层，所有dropouts列表长度内，都加入Dropout层\n",
    "'''\n",
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay): \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #自动保存__init__方法中的所有输入参数到self.hparams中\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1)) # 输出层\n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers) #将所有层组合存放至Sequential\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay #L2正则化系数\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 5 * self.model(x).squeeze(-1)  \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w  # 考虑样本权重的加权MSE\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking: #Sanity Check 调试阶段\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            # r2_val\n",
    "            val_r_square = r2_val(y, prob, weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear() #清空缓存\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #动态优化器，根据验证损失调整学习率。当 val_loss 在 5 个 epoch 内未下降时，学习率减半（factir = 0.5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                                               verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = my_args\n",
    "\n",
    "# checking device\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "loader_device = 'cpu'\n",
    "\n",
    "\n",
    "# Initialize Data Module\n",
    "# forward fill：用前一个有效值填充NaN值，用0填充剩余的NaN值\n",
    "df[xgb_feature_cols] = df[xgb_feature_cols].fillna(method = 'ffill').fillna(0)\n",
    "valid[xgb_feature_cols] = valid[xgb_feature_cols].fillna(method = 'ffill').fillna(0)\n",
    "data_module = DataModule(df, batch_size=args.bs, valid_df=valid, accelerator=loader_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del df\n",
    "gc.collect() #释放删除的df占用的内存\n",
    "torch.set_float32_matmul_precision('medium') #设置 PyTorch 浮点矩阵乘法的计算精度\n",
    "pl.seed_everything(args.seed) #固定所有随机种子，确保实验可复现\n",
    "for fold in range(args.N_fold):\n",
    "    data_module.setup(fold, args.N_fold) #根据当前折数 fold 划分训练集和验证集\n",
    "    # Obtain input dimension\n",
    "    input_dim = data_module.train_dataset.features.shape[1]\n",
    "    # Initialize Model\n",
    "    model = NN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=args.n_hidden,\n",
    "        dropouts=args.dropouts,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    # Initialize Logger\n",
    "    if args.use_wandb:\n",
    "        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "        logger = WandbLogger(experiment=wandb_run)\n",
    "    else:\n",
    "        logger = None\n",
    "    # Initialize Callbacks\n",
    "    early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False) #早停\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") #仅保存最佳模型 \n",
    "    timer = Timer()\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        accelerator=accelerator,\n",
    "        #devices=args.n_gpus,\n",
    "        #strategy='ddp_notebook',\n",
    "        devices=[args.gpuid] if args.usegpu else None,\n",
    "        logger=logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    # Start Training\n",
    "    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n",
    "    # You can find trained best model in your local path\n",
    "    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
